{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Onsala-1 radio spectra obtained from OSO 20m telescope\n",
    "\n",
    "In this notebook, we will analyse a spectrum of Onsala-1 molecular cloud obtained from 20m telescope at OSO. The spectra were measured in radio frequencies and have four emission lines of $CH_3CCH$. The main goal is to find the area under the lines, which is basically the integrated antenna temperature under these lines. With this we can compute the column density of the $CH_3CCH$.\n",
    "\n",
    "The focus of this notebook is Onsala-1. Once we have worked it out, we can make a routine to compute the same for the rest of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import os\n",
    "from glob import glob\n",
    "import astropy.constants as con\n",
    "import utils as utl\n",
    "from scipy.optimize import minimize\n",
    "import dynesty\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from dynesty import plotting as dyplot\n",
    "from dynesty.utils import resample_equal\n",
    "import lmfit\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make a list of all fits file which has spectrum of Onsala-1. We also list the system temperature and the total integration time, which we need to compute the combined/average spectrum (to gain a higher S/N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing all of the fits file\n",
    "f1 = glob(os.getcwd() + '/Data/*.fits')\n",
    "ons, tsys, int_time = np.array([]), np.array([]), np.array([])\n",
    "print('File\\t\\tObject\\t\\tT_sys\\tINTTIME')\n",
    "print('-----------------------------------------------')\n",
    "for i in range(len(f1)):\n",
    "    hdul = fits.open(f1[i])\n",
    "    hdr = hdul[0].header\n",
    "    if hdr['OBJECT'] == 'Onsala 1':\n",
    "        ons = np.hstack((ons, f1[i]))\n",
    "        tsys, int_time = np.hstack((tsys, hdr['TSYS'])),\\\n",
    "             np.hstack((int_time, hdr['INTTIME']))\n",
    "        print(f1[i].split('/')[-1] + '\\t' + hdr['OBJECT'] + '\\t'\\\n",
    "             + str(hdr['TSYS']) + '\\t' + str(hdr['INTTIME']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute an average spectrum out of all of these spectra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zeroth spectra\n",
    "hdul5 = fits.open(ons[0])\n",
    "hdr5, dta5 = hdul5[0].header, hdul5[0].data[0][0]\n",
    "ii5 = np.arange(hdr5['NAXIS1']) + 1\n",
    "freq_all = hdr5['RESTFREQ'] + hdr5['CRVAL1'] + hdr5['CDELT1']*(ii5-hdr5['CRPIX1'])\n",
    "freq_all = int_time[0]*freq_all/tsys[0]\n",
    "temp_all = int_time[0]*dta5/tsys[0]\n",
    "# For all other spectra\n",
    "for i in range(len(ons)-1):\n",
    "    hdul5 = fits.open(ons[i+1])\n",
    "    hdr5, dta5 = hdul5[0].header, hdul5[0].data[0][0]\n",
    "    ii5 = np.arange(hdr5['NAXIS1']) + 1\n",
    "    frq5 = hdr5['RESTFREQ'] + hdr5['CRVAL1'] + hdr5['CDELT1']*(ii5-hdr5['CRPIX1'])\n",
    "    # Saving the data\n",
    "    temp_all = np.vstack((temp_all, int_time[i+1]*dta5/tsys[i+1]))\n",
    "    freq_all = np.vstack((freq_all, int_time[i+1]*frq5/tsys[i+1]))\n",
    "# Weighted average (weighted with int_time/Tsys) over all the spectra\n",
    "freq_avg = np.sum(freq_all, axis=0)/np.sum(int_time/tsys)\n",
    "temp_avg = np.sum(temp_all, axis=0)/np.sum(int_time/tsys)\n",
    "\n",
    "# Plotting the result:\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(freq_avg/1e9, temp_avg, fmt='.', c='orangered')\n",
    "plt.xlabel('Frequency (in GHz)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.xlim([np.min(freq_avg/1e9), np.max(freq_avg/1e9)])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! This is a nice spectrum of Onsala 1!\n",
    "We can first convert frequency to the velocity first -- to do this we can use the following Doppler formula:\n",
    "\n",
    "$$v = c \\cdot \\frac{f_s - f_o}{f_s}$$\n",
    "\n",
    "The symbols have their usual meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the rest frame frequency\n",
    "diff = freq_avg - hdr5['OBSFREQ']\n",
    "rest_freq = hdr5['RESTFREQ']#freq_avg + diff\n",
    "# To find the velocity of the target!\n",
    "velo_avg = con.c.value*(rest_freq - freq_avg)/rest_freq\n",
    "velo_avg = velo_avg + hdr5['VLSR']# + hdr5['VELO-GEO'] + hdr5['VELO-HEL']\n",
    "# We can plot the results\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(velo_avg/1e3, temp_avg, fmt='.', c='orangered')\n",
    "plt.xlabel('Velocity (in km/s)')\n",
    "plt.ylabel('Temperature (in K)')\n",
    "plt.xlim([np.min(velo_avg/1e3), np.max(velo_avg/1e3)])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the uncertainties on these points\n",
    "\n",
    "One peculier thing about this spectrum is that its points do not have errors on them. We all know that errors are fundamentals to any spectrum --- but, how do we estimate the unceratainties? One way out could be to evaluate the standard deviation of the _whole_ spectrum and treat it as a stadard deviation of a Gaussian distribution with zero mean, which then makes an array of uncertainties.\n",
    "\n",
    "We can do this bacause the spctral lines are like outliers to the continumm (see, the histogram of the spectrum below), which is basically a noise.\n",
    "\n",
    "Below we implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "plt.hist(temp_avg, bins=100, histtype='step')\n",
    "plt.xlabel('Temperature (in K)')\n",
    "plt.ylabel('Counts')\n",
    "temp_err = np.abs(np.random.normal(0, np.std(temp_avg), len(temp_avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how the final spectrum would look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the results\n",
    "## Binned data\n",
    "velo_bin, temp_bin, temp_err_bin, _ = utl.lcbin(velo_avg, temp_avg, binwidth=500)\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(velo_avg/1e3, temp_avg, yerr=temp_err, fmt='.', c='orangered', alpha=0.1)\n",
    "plt.errorbar(velo_bin/1e3, temp_bin, yerr=temp_err_bin, fmt='o', mfc='white', c='black')\n",
    "plt.xlabel('Velocity (in km/s)')\n",
    "plt.ylabel('Temperature (in K)')\n",
    "plt.xlim([0, 90])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the fitting...\n",
    "\n",
    "So, there are four lines (though we cannot see them individually) along with some offset trend. What we can do is to build a `lmfit` model that has four gaussian models and a linear model. Then we can first use `lmfit` to fit it to the data, and then using `emcee` method to explore the posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sig, amp):\n",
    "    return amp*np.exp(-0.5 * ((x-mu)/sig)**2)\n",
    "def line(x,m,c):\n",
    "    return m*x + c\n",
    "\n",
    "# Adding our initial guesses\n",
    "pars = lmfit.Parameters()\n",
    "pars.add_many(('m', 0., True), ('c', 0.1, True),\\\n",
    "    ('mu1', 11000., True), ('sig1', 1500., True), ('amp1', 0.4, True),\\\n",
    "    ('mu2', 18000., True), ('sig2', 1500., True), ('amp2', 0.3, True),\\\n",
    "    ('mu3', 35000., True), ('sig3', 1500., True), ('amp3', 0.13, True),\\\n",
    "    ('mu4', 62000., True), ('sig4', 1500., True), ('amp4', 0.1, True))\n",
    "\n",
    "def tot_model(x, p):\n",
    "    v = p.valuesdict()\n",
    "    model_fin = gaussian(x, v['mu1'], v['sig1'], v['amp1']) +\\\n",
    "        gaussian(x, v['mu2'], v['sig2'], v['amp2'])+\\\n",
    "        gaussian(x, v['mu3'], v['sig3'], v['amp3'])+\\\n",
    "        gaussian(x, v['mu4'], v['sig4'], v['amp4'])+\\\n",
    "        line(x, v['m'], v['c'])\n",
    "    return model_fin\n",
    "\n",
    "def residual(p):\n",
    "    model_fin = tot_model(velo_avg, p)\n",
    "    chi2 = model_fin-temp_avg#)/temp_err\n",
    "    return chi2\n",
    "\n",
    "def residual_wt(p):\n",
    "    model_fin = tot_model(velo_avg, p)\n",
    "    chi2 = (model_fin-temp_avg)/temp_err\n",
    "    return chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `lmfit.minimize` to find the \"first\" best-fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = lmfit.minimize(residual, pars, method='nelder', nan_policy='omit')\n",
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vall = np.linspace(np.min(velo_avg), np.max(velo_avg), 10000)\n",
    "best_fit = tot_model(vall, mi.params)\n",
    "\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(velo_avg/1e3, temp_avg, yerr=temp_err, fmt='.', c='orangered', alpha=0.1)\n",
    "plt.errorbar(velo_bin/1e3, temp_bin, yerr=temp_err_bin, fmt='o', mfc='white', c='black')\n",
    "plt.plot(vall/1e3, best_fit, c='cornflowerblue', lw=3, zorder=5)\n",
    "plt.xlabel('Velocity (in km/s)')\n",
    "plt.ylabel('Temperature (in K)')\n",
    "plt.xlim([0, 90])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lmfit.minimize(residual_wt, method='emcee', nan_policy='omit', burn=10000, steps=50000, thin=20,\n",
    "                     params=mi.params, is_weighted=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emcee_plot = corner.corner(res.flatchain, labels=res.var_names,\n",
    "                           truths=list(res.params.valuesdict().values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in range(len(res.var_names)):\n",
    "    data[res.var_names[i]] = np.asarray(res.flatchain[res.var_names[i]])\n",
    "pickle.dump(data,open('Onsala1_emcee.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a116efe97cecb71d500e659e06ef5c8b27a2e41c6ffe630e19c5aa9699e861e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
