{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Onsala-1 radio spectra obtained from OSO 20m telescope\n",
    "\n",
    "In this notebook, we will analyse a spectrum of Onsala-1 molecular cloud obtained from 20m telescope at OSO. The spectra were measured in radio frequencies and have four emission lines of $CH_3CCH$. The main goal is to find the area under the lines, which is basically the integrated antenna temperature under these lines. With this we can compute the column density of the $CH_3CCH$.\n",
    "\n",
    "The focus of this notebook is Onsala-1. Once we have worked it out, we can make a routine to compute the same for the rest of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import emcee\n",
    "import os\n",
    "from glob import glob\n",
    "import astropy.constants as con\n",
    "import utils as utl\n",
    "from scipy.optimize import minimize\n",
    "import corner\n",
    "import dynesty\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from dynesty import plotting as dyplot\n",
    "from dynesty.utils import resample_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make a list of all fits file which has spectrum of Onsala-1. We also list the system temperature and the total integration time, which we need to compute the combined/average spectrum (to gain a higher S/N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing all of the fits file\n",
    "f1 = glob(os.getcwd() + '/Data/*.fits')\n",
    "ons, tsys, int_time = np.array([]), np.array([]), np.array([])\n",
    "print('File\\t\\tObject\\t\\tT_sys\\tINTTIME')\n",
    "print('-----------------------------------------------')\n",
    "for i in range(len(f1)):\n",
    "    hdul = fits.open(f1[i])\n",
    "    hdr = hdul[0].header\n",
    "    if hdr['OBJECT'] == 'Onsala 1':\n",
    "        ons = np.hstack((ons, f1[i]))\n",
    "        tsys, int_time = np.hstack((tsys, hdr['TSYS'])),\\\n",
    "             np.hstack((int_time, hdr['INTTIME']))\n",
    "        print(f1[i].split('/')[-1] + '\\t' + hdr['OBJECT'] + '\\t'\\\n",
    "             + str(hdr['TSYS']) + '\\t' + str(hdr['INTTIME']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute an average spectrum out of all of these spectra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For zeroth spectra\n",
    "hdul5 = fits.open(ons[0])\n",
    "hdr5, dta5 = hdul5[0].header, hdul5[0].data[0][0]\n",
    "ii5 = np.arange(hdr5['NAXIS1']) + 1\n",
    "freq_all = hdr5['RESTFREQ'] + hdr5['CRVAL1'] + hdr5['CDELT1']*(ii5-hdr5['CRPIX1'])\n",
    "freq_all = int_time[0]*freq_all/tsys[0]\n",
    "temp_all = int_time[0]*dta5/tsys[0]\n",
    "# For all other spectra\n",
    "for i in range(len(ons)-1):\n",
    "    hdul5 = fits.open(ons[i+1])\n",
    "    hdr5, dta5 = hdul5[0].header, hdul5[0].data[0][0]\n",
    "    ii5 = np.arange(hdr5['NAXIS1']) + 1\n",
    "    frq5 = hdr5['RESTFREQ'] + hdr5['CRVAL1'] + hdr5['CDELT1']*(ii5-hdr5['CRPIX1'])\n",
    "    # Saving the data\n",
    "    temp_all = np.vstack((temp_all, int_time[i+1]*dta5/tsys[i+1]))\n",
    "    freq_all = np.vstack((freq_all, int_time[i+1]*frq5/tsys[i+1]))\n",
    "# Weighted average (weighted with int_time/Tsys) over all the spectra\n",
    "freq_avg = np.sum(freq_all, axis=0)/np.sum(int_time/tsys)\n",
    "temp_avg = np.sum(temp_all, axis=0)/np.sum(int_time/tsys)\n",
    "\n",
    "# Plotting the result:\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(freq_avg/1e9, temp_avg, fmt='.', c='orangered')\n",
    "plt.xlabel('Frequency (in GHz)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.xlim([np.min(freq_avg/1e9), np.max(freq_avg/1e9)])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! This is a nice spectrum of Onsala 1!\n",
    "We can first convert frequency to the velocity first -- to do this we can use the following Doppler formula:\n",
    "\n",
    "$$v = c \\cdot \\frac{f_s - f_o}{f_s}$$\n",
    "\n",
    "The symbols have their usual meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the rest frame frequency\n",
    "diff = freq_avg - hdr5['OBSFREQ']\n",
    "rest_freq = hdr5['RESTFREQ']#freq_avg + diff\n",
    "# To find the velocity of the target!\n",
    "velo_avg = con.c.value*(rest_freq - freq_avg)/rest_freq\n",
    "velo_avg = velo_avg + hdr5['VLSR']# + hdr5['VELO-GEO'] + hdr5['VELO-HEL']\n",
    "# We can plot the results\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(velo_avg/1e3, temp_avg, fmt='.', c='orangered')\n",
    "plt.xlabel('Velocity (in km/s)')\n",
    "plt.ylabel('Temperature (in K)')\n",
    "plt.xlim([np.min(velo_avg/1e3), np.max(velo_avg/1e3)])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the uncertainties on these points\n",
    "\n",
    "One peculier thing about this spectrum is that its points do not have errors on them. We all know that errors are fundamentals to any spectrum --- but, how do we estimate the unceratainties? One way out could be to evaluate the standard deviation of the _whole_ spectrum and treat it as a stadard deviation of a Gaussian distribution with zero mean, which then makes an array of uncertainties.\n",
    "\n",
    "We can do this bacause the spctral lines are like outliers to the continumm (see, the histogram of the spectrum below), which is basically a noise.\n",
    "\n",
    "Below we implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 8))\n",
    "plt.hist(temp_avg, bins=100, histtype='step')\n",
    "plt.xlabel('Temperature (in K)')\n",
    "plt.ylabel('Counts')\n",
    "temp_err = np.abs(np.random.normal(0, np.std(temp_avg), len(temp_avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how the final spectrum would look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the results\n",
    "## Binned data\n",
    "velo_bin, temp_bin, temp_err_bin, _ = utl.lcbin(velo_avg, temp_avg, binwidth=500)\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(velo_avg/1e3, temp_avg, yerr=temp_err, fmt='.', c='orangered', alpha=0.1)\n",
    "plt.errorbar(velo_bin/1e3, temp_bin, yerr=temp_err_bin, fmt='o', mfc='white', c='black')\n",
    "plt.xlabel('Velocity (in km/s)')\n",
    "plt.ylabel('Temperature (in K)')\n",
    "plt.xlim([0, 90])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the fitting...\n",
    "\n",
    "So, there are four lines (though we cannot see them individually) along with some offset trend. What we can do is to build a model that has four gaussian models and a linear model. Then we can use `dynesty` to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, m, c, am1, am2, am3, am4, mu1, mu2, mu3, mu4, si1, si2, si3, si4):\n",
    "    ln1 = utl.line(x, m, c)\n",
    "    gau1 = utl.gaus(x, am1, mu1, si1)\n",
    "    gau2 = utl.gaus(x, am2, mu2, si2)\n",
    "    gau3 = utl.gaus(x, am3, mu3, si3)\n",
    "    gau4 = utl.gaus(x, am4, mu4, si4)\n",
    "    return ln1 + gau1 + gau2 + gau3 + gau4\n",
    "\n",
    "def chi_sqrd(x):\n",
    "    #global velo_avg, temp_avg, temp_err\n",
    "    m, c, am1, am2, am3, am4, mu1, mu2, mu3, mu4, si1, si2, si3, si4 = x\n",
    "    mod = model(velo_avg, m, c, am1, am2, am3, am4, mu1, mu2, mu3, mu4, si1, si2, si3, si4)\n",
    "    chi2 = (temp_avg - mod)**2\n",
    "    return 0.5*np.sum(chi2)/2\n",
    "\n",
    "def log_like(x):\n",
    "    #global velo_avg, temp_avg, temp_err\n",
    "    m, c, am1, am2, am3, am4, mu1, mu2, mu3, mu4, si1, si2, si3, si4 = x\n",
    "    mods = model(velo_avg, m, c, am1, am2, am3, am4, mu1, mu2, mu3, mu4, si1, si2, si3, si4)\n",
    "    resid = temp_avg-mods\n",
    "    chi2 = np.sum(-resid**2/(2*temp_err*temp_err))\n",
    "    errs = np.sum(-np.log(temp_err))\n",
    "    return chi2+errs\n",
    "\n",
    "def neg_loglike(x):\n",
    "    #global velo_avg, temp_avg, temp_err\n",
    "    m, c, am1, am2, am3, am4, mu1, mu2, mu3, mu4, si1, si2, si3, si4 = x\n",
    "    mods = model(velo_avg, m, c, am1, am2, am3, am4, mu1, mu2, mu3, mu4, si1, si2, si3, si4)\n",
    "    resid = temp_avg-mods\n",
    "    chi2 = np.sum(-resid**2/(2*temp_err*temp_err))\n",
    "    errs = np.sum(-np.log(temp_err))\n",
    "    return -chi2-errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `scipy.optimize.minimize` to find the \"first\" best-fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinit = np.array([0., 0.1, 0.4, 0.3, 0.13, 0.1,\\\n",
    "     11000, 18000, 35000, 62000, 1.5*1000, 1.5*1000, 1.5*1000, 1.5*1000])\n",
    "soln = minimize(chi_sqrd, x0=xinit, method='BFGS')\n",
    "\n",
    "vall = np.linspace(np.min(velo_avg), np.max(velo_avg), 10000)\n",
    "best_fit = model(vall, *soln.x)\n",
    "\n",
    "plt.figure(figsize=(16/1.5, 9/1.5))\n",
    "plt.errorbar(velo_avg/1e3, temp_avg, yerr=temp_err, fmt='.', c='orangered', alpha=0.1)\n",
    "plt.errorbar(velo_bin/1e3, temp_bin, yerr=temp_err_bin, fmt='o', mfc='white', c='black')\n",
    "plt.plot(vall/1e3, best_fit, c='cornflowerblue', lw=3, zorder=5)\n",
    "plt.xlabel('Velocity (in km/s)')\n",
    "plt.ylabel('Temperature (in K)')\n",
    "plt.xlim([0, 90])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the results:\n",
    "print(soln.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `dynesty` to perform a more robust analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(t, a, b):\n",
    "    return (b-a)*t + a\n",
    "\n",
    "def prior_transform(ux):\n",
    "    x = np.copy(ux)\n",
    "    # Linear function:\n",
    "    x[0], x[1] = uniform(ux[0], -0.01, 0.01),\\\n",
    "        stats.loguniform.ppf(ux[1], a=1e-2, b=5e-1)\n",
    "    # Amplitudes:\n",
    "    x[2], x[3], x[4], x[5] = stats.loguniform.ppf(ux[2], a=1e-2, b=1.), stats.loguniform.ppf(ux[3], a=1e-2, b=1.),\\\n",
    "         stats.loguniform.ppf(ux[4], a=1e-2, b=1.), stats.loguniform.ppf(ux[5], a=1e-2, b=1.)\n",
    "    # Central positions:\n",
    "    x[6], x[7], x[8], x[9] = stats.norm.ppf(ux[6], loc=soln.x[6], scale=5e3), stats.norm.ppf(ux[7], loc=soln.x[7], scale=5e3),\\\n",
    "         stats.norm.ppf(ux[8], loc=soln.x[8], scale=5e3), stats.norm.ppf(ux[9], loc=soln.x[9], scale=5e3)\n",
    "    # Widths:\n",
    "    x[10], x[11], x[12], x[13] = stats.norm.ppf(ux[10], loc=soln.x[10], scale=1e2), stats.norm.ppf(ux[11], loc=soln.x[11], scale=1e2),\\\n",
    "         stats.norm.ppf(ux[12], loc=soln.x[12], scale=1e2), stats.norm.ppf(ux[13], loc=soln.x[13], scale=1e2)\n",
    "    return x\n",
    "\n",
    "def prior_tight(x):\n",
    "    y = []\n",
    "    for i in range(len(soln.x)):\n",
    "        y.append(stats.norm.ppf(x[i], loc=soln.x[i], scale=soln.x[i]/10))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsampler = dynesty.DynamicNestedSampler(loglikelihood=log_like, prior_transform=prior_transform,\\\n",
    "    ndim=14, nlive=1000, bound='multi', sample='rwalk')\n",
    "dsampler.run_nested()\n",
    "dres = dsampler.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Keys: ', dres.keys())\n",
    "dres.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r'$m$', r'$c$', r'$a_1$', r'$a_2$', r'$a_3$', r'$a_4$',\\\n",
    "    r'$\\mu_1$', r'$\\mu_2$', r'$\\mu_3$', r'$\\mu_4$',\\\n",
    "    r'$\\sigma_1$', r'$\\sigma_2$', r'$\\sigma_3$', r'$\\sigma_4$']\n",
    "\n",
    "fig, axes = dyplot.traceplot(dres, labels=labels)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = dyplot.cornerplot(dres, show_titles=True, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytic evidence solution\n",
    "lnz_truth = 32000. * -np.log(2 * 10.)  # log(volume) of prior; log(like) is normalized\n",
    "fig, axes = dyplot.runplot(dres, color='blue', lnz_truth=lnz_truth, truth_color='black')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.exp(dres['logwt'] - dres['logz'][-1])\n",
    "posterior_samples = resample_equal(dres.samples, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = ['m', 'c', 'a_1', 'a_2', 'a_3', 'a_4',\\\n",
    "    'mu_1', 'mu_2', 'mu_3', 'mu_4', 'sigma_1', 'sigma_2', 'sigma_3', 'sigma_4']\n",
    "\n",
    "post_samps = {}\n",
    "for i in range(len(labels1)):\n",
    "    post_samps[labels1] = posterior_samples[:, i]\n",
    "\n",
    "# Dumping a pickle\n",
    "pickle.dump(post_samps,open('Onsala1.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "825b35cec348bfd604c386872633be2ab3005509c7104696a60c783e48e695c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dynesty12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
